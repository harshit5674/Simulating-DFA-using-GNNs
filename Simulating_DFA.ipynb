{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LhBmPKT7Ntbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f177ec-009a-44f5-95d0-f705b59f22e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "R6XhCIiuOF3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d2e9b7-7263-41df-931b-dad820aa5bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spektral in /usr/local/lib/python3.8/dist-packages (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from spektral) (2.8.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from spektral) (4.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from spektral) (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from spektral) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from spektral) (1.7.3)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from spektral) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from spektral) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from spektral) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from spektral) (1.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (14.0.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.51.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (21.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (0.28.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.12)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2.0->spektral) (2.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (2.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->spektral) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->spektral) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->spektral) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install spektral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JibCfYEF6zmx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import Dataset, DisjointLoader, Graph\n",
        "from spektral.layers import GCSConv, GlobalAvgPool\n",
        "from spektral.transforms.normalize_adj import NormalizeAdj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vS4c3o4zoFOq"
      },
      "outputs": [],
      "source": [
        "!rm -r YALE-RESEARCH-WORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sy0sVbBy68u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04dfde9-a193-4acf-82e4-41cf7546d9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YALE-RESEARCH-WORK'...\n",
            "remote: Enumerating objects: 345089, done.\u001b[K\n",
            "remote: Total 345089 (delta 0), reused 0 (delta 0), pack-reused 345089\u001b[K\n",
            "Receiving objects: 100% (345089/345089), 128.51 MiB | 27.05 MiB/s, done.\n",
            "Resolving deltas: 100% (345032/345032), done.\n",
            "Checking out files: 100% (70013/70013), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/harshit5674/YALE-RESEARCH-WORK.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WBPl9EIx9Wu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d12f0f-1214-4883-d61f-981cf17616ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  YALE-RESEARCH-WORK\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Jq84bhDU-SYu"
      },
      "outputs": [],
      "source": [
        "path=\"YALE-RESEARCH-WORK/dataset_automata\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZLrwzw439Y2O"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "      \n",
        "    def read(self):\n",
        "      output=[]\n",
        "      for i in range(0,70000):\n",
        "         data = np.load(os.path.join(path, f'graph_{i}.npz'))\n",
        "         output.append(Graph(x=data['x'], a=data['a'], e=data['e'],y=data['y']))\n",
        "        \n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset1(Dataset):\n",
        "    \n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "      \n",
        "    def read(self):\n",
        "      output=[]\n",
        "      for i in range(0,70000):\n",
        "         data = np.load(os.path.join(path, f'graph_{i}.npz'))\n",
        "         output.append(Graph(x=data['x'], a=data['a'], y=data['y']))\n",
        "        \n",
        "      return output"
      ],
      "metadata": {
        "id": "7yHW8Vjz1zym"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xVnEin3q9kt5"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SVCtPEY99uHb"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset(transforms=NormalizeAdj()) \n",
        "idxs = np.random.permutation(len(dataset))\n",
        "split = int(0.85 * len(dataset))\n",
        "idx_tr, idx_te = np.split(idxs, [split])\n",
        "dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ejTP4hziCjp3"
      },
      "outputs": [],
      "source": [
        "path=\"YALE-RESEARCH-WORK/dataset_automata\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nPVo6bvqAOzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556cfd4a-a5d7-44da-f607-4bc822dee08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  np.random.shuffle(a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 64.33604431152344\n",
            "Loss: 58.70257568359375\n",
            "Loss: 57.860450744628906\n",
            "Loss: 57.064918518066406\n",
            "Loss: 56.09347152709961\n",
            "Loss: 55.27485656738281\n",
            "Loss: 54.91847229003906\n",
            "Loss: 54.5775146484375\n",
            "Loss: 54.4722900390625\n",
            "Loss: 54.287750244140625\n",
            "Testing model\n",
            "Done. Test loss: 53.94426727294922\n",
            "2303\n",
            "3380\n",
            "7772\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10  # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = ECCConv(32, activation=\"relu\")\n",
        "        self.conv2 = ECCConv(32, activation=\"relu\")\n",
        "        self.conv3=ECCConv(32,activation=\"relu\")\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a, e])\n",
        "        x = self.conv2([x, a, e])\n",
        "        x = self.conv3([x,a,e])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "  \n",
        "  inputs, target = batch\n",
        "  predictions = model(inputs, training=False)\n",
        "  loss += loss_fn(target, predictions)\n",
        "  for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "       # print(er[-1])\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SyFd4AK4Gcj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88582e08-869b-4cf8-e6ba-1d5033de88bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_4:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_7/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_8:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_7:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_7/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_11:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_10:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_7/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_13:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_12:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_7/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_7/Reshape_2:0\", shape=(None, 3, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_7/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_4:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_6/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_8:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_7:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_6/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_11:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_10:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_6/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_13:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_12:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_6/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_6/Reshape_2:0\", shape=(None, 3, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_6/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_4:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_5/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_8:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_7:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_5/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_11:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_10:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_5/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_13:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_12:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_5/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_11/gat_conv_5/Reshape_2:0\", shape=(None, 3, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_11/gat_conv_5/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 71.6545181274414\n",
            "Loss: 68.27639770507812\n",
            "Loss: 67.35057067871094\n",
            "Loss: 67.00225830078125\n",
            "Loss: 66.67135620117188\n",
            "Loss: 66.2906723022461\n",
            "Loss: 66.27021026611328\n",
            "Loss: 65.97164154052734\n",
            "Loss: 65.71961212158203\n",
            "Loss: 65.64010620117188\n",
            "Testing model\n",
            "Done. Test loss: 70.5650405883789\n",
            "1113\n",
            "3380\n",
            "6976\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10  # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(32, attn_heads=3, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=\"relu\", use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None)\n",
        "        self.conv2 =GATConv(32, attn_heads=3, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=\"relu\", use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform')\n",
        "        self.conv3 =GATConv(32, attn_heads=3, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=\"relu\", use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform')\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_UK7Wrd71Xpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80d9524-073f-4653-934b-450e05906cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 65.84223175048828\n",
            "Loss: 60.28924560546875\n",
            "Loss: 59.26241683959961\n",
            "Loss: 58.956634521484375\n",
            "Loss: 58.6509895324707\n",
            "Loss: 58.60116195678711\n",
            "Loss: 58.41450500488281\n",
            "Loss: 58.276371002197266\n",
            "Loss: 58.2286262512207\n",
            "Loss: 58.14802169799805\n",
            "Testing model\n",
            "Done. Test loss: 57.23370361328125\n",
            "1921\n",
            "3380\n",
            "7474\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool,MessagePassing\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10  # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = ECCConv(32, activation=\"relu\")\n",
        "        self.conv2 = ECCConv(32, activation=\"relu\")\n",
        "        self.conv3=MessagePassing(aggregate='sum')\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a, e])\n",
        "        x = self.conv2([x, a, e])\n",
        "        x = self.conv3([x,a,e])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "  \n",
        "  inputs, target = batch\n",
        "  predictions = model(inputs, training=False)\n",
        "  loss += loss_fn(target, predictions)\n",
        "  for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "       # print(er[-1])\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vomtSydE1oDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5518f71f-7c10-48d2-c0c0-8118fcb5ee2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_4:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_9/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_8:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_7:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_9/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_11:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_10:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_9/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_13:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_12:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_9/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_9/Reshape_2:0\", shape=(None, 3, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_9/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_4:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_8/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_8:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_7:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_8/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_11:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_10:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_8/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_13:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_12:0\", shape=(None, 3), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_8/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_13/gat_conv_8/Reshape_2:0\", shape=(None, 3, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_13/gat_conv_8/Cast:0\", shape=(3,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 71.04652404785156\n",
            "Loss: 68.45233154296875\n",
            "Loss: 67.75459289550781\n",
            "Loss: 67.03807830810547\n",
            "Loss: 66.7838134765625\n",
            "Loss: 66.44432830810547\n",
            "Loss: 65.97178649902344\n",
            "Loss: 65.88166046142578\n",
            "Loss: 65.78783416748047\n",
            "Loss: 65.5538558959961\n",
            "Testing model\n",
            "Done. Test loss: 67.02734375\n",
            "1773\n",
            "3380\n",
            "6617\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10  # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(32, attn_heads=3, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=\"relu\", use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, attn_kernel_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, attn_kernel_constraint=None)\n",
        "        self.conv2 =GATConv(32, attn_heads=3, concat_heads=True, dropout_rate=0.5, return_attn_coef=False, add_self_loops=True, activation=\"relu\", use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', attn_kernel_initializer='glorot_uniform')\n",
        "        self.conv3=MessagePassing(aggregate='sum')\n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Rb6o7sL6Q3n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8873ad-74da-4bfe-9730-fe88740e88be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_14/agnn_conv_7/Reshape_4:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_14/agnn_conv_7/Reshape_3:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_14/agnn_conv_7/Cast_1:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_14/agnn_conv_7/Reshape_7:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_14/agnn_conv_7/Reshape_6:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_14/agnn_conv_7/Cast_2:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_14/agnn_conv_6/Reshape_4:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_14/agnn_conv_6/Reshape_3:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_14/agnn_conv_6/Cast_1:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_14/agnn_conv_6/Reshape_7:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_14/agnn_conv_6/Reshape_6:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_14/agnn_conv_6/Cast_2:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_14/agnn_conv_5/Reshape_2:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_14/agnn_conv_5/Reshape_1:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_14/agnn_conv_5/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_14/agnn_conv_5/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_14/agnn_conv_5/Reshape_4:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_14/agnn_conv_5/Cast_1:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 97.63720703125\n",
            "Loss: 76.5283432006836\n",
            "Loss: 73.88966369628906\n",
            "Loss: 72.96376037597656\n",
            "Loss: 72.45005798339844\n",
            "Loss: 72.14883422851562\n",
            "Loss: 71.93587493896484\n",
            "Loss: 71.77703094482422\n",
            "Loss: 71.61277770996094\n",
            "Loss: 71.48017120361328\n",
            "Testing model\n",
            "Done. Test loss: 70.46073913574219\n",
            "934\n",
            "3380\n",
            "6691\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=AGNNConv(trainable=True, aggregate='sum', activation=None)\n",
        "        self.conv2=AGNNConv(trainable=True, aggregate='sum', activation=None)\n",
        "        self.conv3=AGNNConv(trainable=True, aggregate='sum', activation=None)\n",
        "\n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kbcFILEuEQZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827c8f25-8f8d-420b-a748-06978964d31b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_15/agnn_conv_9/Reshape_4:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_15/agnn_conv_9/Reshape_3:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_15/agnn_conv_9/Cast_1:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_15/agnn_conv_9/Reshape_7:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_15/agnn_conv_9/Reshape_6:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_15/agnn_conv_9/Cast_2:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_15/agnn_conv_8/Reshape_2:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_15/agnn_conv_8/Reshape_1:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_15/agnn_conv_8/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/net_15/agnn_conv_8/Reshape_5:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/net_15/agnn_conv_8/Reshape_4:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradient_tape/net_15/agnn_conv_8/Cast_1:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 90.60151672363281\n",
            "Loss: 75.89082336425781\n",
            "Loss: 74.1084976196289\n",
            "Loss: 73.11143493652344\n",
            "Loss: 72.56303405761719\n",
            "Loss: 72.25262451171875\n",
            "Loss: 72.0198745727539\n",
            "Loss: 71.82147979736328\n",
            "Loss: 71.64606475830078\n",
            "Loss: 71.48455810546875\n",
            "Testing model\n",
            "Done. Test loss: 70.55004119873047\n",
            "964\n",
            "3380\n",
            "6751\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=AGNNConv(trainable=True, aggregate='sum', activation=None)\n",
        "        self.conv2=AGNNConv(trainable=True, aggregate='sum', activation=None)\n",
        "        self.conv3=MessagePassing(aggregate='sum')\n",
        "\n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XEnsCVR4ZXOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eecd1fa-bfaa-45e0-aaf3-56f7e5c38e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 66.09622192382812\n",
            "Loss: 64.6649169921875\n",
            "Loss: 64.37018585205078\n",
            "Loss: 64.12459564208984\n",
            "Loss: 63.964515686035156\n",
            "Loss: 63.688297271728516\n",
            "Loss: 63.611759185791016\n",
            "Loss: 63.49766159057617\n",
            "Loss: 63.413818359375\n",
            "Loss: 63.29599380493164\n",
            "Testing model\n",
            "Done. Test loss: 62.629600524902344\n",
            "1432\n",
            "3380\n",
            "7011\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv,GeneralConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv2=GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv3=GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "\n",
        "        \n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uosl_Rfla2x8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3ff89a-b74b-4249-c0a1-5b8ce8b7e257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 69.73238372802734\n",
            "Loss: 66.16064453125\n",
            "Loss: 65.68498229980469\n",
            "Loss: 65.2615737915039\n",
            "Loss: 64.96328735351562\n",
            "Loss: 64.62937927246094\n",
            "Loss: 64.33808898925781\n",
            "Loss: 64.05093383789062\n",
            "Loss: 63.940956115722656\n",
            "Loss: 63.80311584472656\n",
            "Testing model\n",
            "Done. Test loss: 63.41973876953125\n",
            "1722\n",
            "3380\n",
            "6848\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv,GeneralConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv2=GeneralConv(channels=256, batch_norm=True, dropout=0.0, aggregate='sum', activation='prelu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv3=MessagePassing(aggregate='sum')\n",
        "\n",
        "        \n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pcJ4ZipMZ_wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4cc01c-a6e4-420d-a1ef-83f403095dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 67.85474395751953\n",
            "Loss: 64.70655059814453\n",
            "Loss: 64.209228515625\n",
            "Loss: 63.847625732421875\n",
            "Loss: 63.48682403564453\n",
            "Loss: 63.2465934753418\n",
            "Loss: 62.93763732910156\n",
            "Loss: 62.77790832519531\n",
            "Loss: 62.532508850097656\n",
            "Loss: 62.342811584472656\n",
            "Testing model\n",
            "Done. Test loss: 61.57862854003906\n",
            "1184\n",
            "3380\n",
            "7242\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv,GeneralConv,GraphSageConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=GraphSageConv(channels=256, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv2=GraphSageConv(channels=256, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv3=GraphSageConv(channels=256, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "USV1ePN4aq9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d976795b-08a1-4431-dae7-696dfdbdf365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 66.97750091552734\n",
            "Loss: 65.23843383789062\n",
            "Loss: 64.8147201538086\n",
            "Loss: 64.45862579345703\n",
            "Loss: 64.13648986816406\n",
            "Loss: 63.899169921875\n",
            "Loss: 63.64555740356445\n",
            "Loss: 63.49897766113281\n",
            "Loss: 63.214271545410156\n",
            "Loss: 63.00121307373047\n",
            "Testing model\n",
            "Done. Test loss: 61.96047592163086\n",
            "1354\n",
            "3380\n",
            "7138\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv,GeneralConv,GraphSageConv\n",
        "from spektral.models.gcn import GCN\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=GraphSageConv(channels=256, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv2=GraphSageConv(channels=256, aggregate='mean', activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "        self.conv3=MessagePassing(aggregate='sum')\n",
        "\n",
        "\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(n_out)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e, i = inputs\n",
        "        x = self.conv1([x, a])\n",
        "        x = self.conv2([x, a])\n",
        "        x = self.conv3([x, a])\n",
        "        output = self.global_pool([x, i])\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset1(transforms=NormalizeAdj()) \n",
        "idxs = np.random.permutation(len(dataset))\n",
        "split = int(0.85 * len(dataset))\n",
        "idx_tr, idx_te = np.split(idxs, [split])\n",
        "dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]"
      ],
      "metadata": {
        "id": "JRdW0DOn16EI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "s8yhDGl7aIJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d60095-2357-462a-f876-56cca05653f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset1' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  np.random.shuffle(a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 127.97563171386719\n",
            "Loss: 105.89288330078125\n",
            "Loss: 91.37091064453125\n",
            "Loss: 82.13446044921875\n",
            "Loss: 76.80769348144531\n",
            "Loss: 73.62044525146484\n",
            "Loss: 71.12588500976562\n",
            "Loss: 68.99361419677734\n",
            "Loss: 67.03250122070312\n",
            "Loss: 65.2644271850586\n",
            "Testing model\n",
            "Done. Test loss: 59.2122917175293\n",
            "1232\n",
            "3435\n",
            "7657\n",
            "10500\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GATConv, MessagePassing, AGNNConv,GeneralConv,GraphSageConv\n",
        "from spektral.models.general_gnn import GeneralGNN\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3  # Learning rate\n",
        "epochs = 10 # Number of training epochs\n",
        "batch_size = 32  # Batch size\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "################################################################################\n",
        "\n",
        "model = GeneralGNN(output=44, activation=None, hidden=256, message_passing=4, pre_process=2, post_process=2, connectivity='cat', batch_norm=True, dropout=0.0, aggregate='sum', hidden_activation='prelu', pool='sum')\n",
        "\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn = MeanSquaredError()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    \n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "count=0\n",
        "total_count=0\n",
        "ov_count=0\n",
        "ov_total_count=0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    for i in range(predictions.shape[0]):\n",
        "      er=predictions[i]\n",
        "      we=target[i]\n",
        "      ok=0\n",
        "      if er[-1]<=40:\n",
        "        ok=0\n",
        "      else:\n",
        "        ok=100\n",
        "      if ok==we[-1] and ok==100:\n",
        "        count=count+1\n",
        "      if we[-1]==100:\n",
        "        total_count=total_count+1\n",
        "      if we[-1]==ok:\n",
        "        ov_count=ov_count+1\n",
        "      ov_total_count=ov_total_count+1\n",
        "    loss += loss_fn(target, predictions)\n",
        "loss /= loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))\n",
        "print(count)\n",
        "print(total_count)\n",
        "print(ov_count)\n",
        "print(ov_total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmhVvjFme4MF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}